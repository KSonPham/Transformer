{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import math as m\n",
    "import warnings\n",
    "import copy\n",
    "from data import Data\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "torch.manual_seed(0)\n",
    "RUN_EXAMPLES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"layernorm_eps\": 5e-7, \n",
    "          \"d_model\": 512,\n",
    "          \"hidden_size\": 2048,\n",
    "          \"dropout\": 0.1,\n",
    "          \"n_heads\": 8,\n",
    "          \"n_layers\": 6,\n",
    "          \"batch_size\": 128,\n",
    "          \"max_length\": 30,\n",
    "          \"warmup\": 3000,\n",
    "          \"base_lr\": 0.5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model = config[\"d_model\"]):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        embedding = self.embedding(x)\n",
    "        return embedding * m.sqrt(self.d_model)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model = config[\"d_model\"], dropout = config[\"dropout\"], max_len=config[\"max_length\"]):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * -(m.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, : x.size(1)].requires_grad_(False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, n_heads = config[\"n_heads\"], d_model = config[\"d_model\"]):\n",
    "        super().__init__()\n",
    "        self.Qs = []\n",
    "        self.Ks = []\n",
    "        self.Vs = []\n",
    "        self.d_model = d_model\n",
    "        self.n_head = n_heads\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "        self.Qs = nn.Linear(d_model,d_model,bias=True)\n",
    "        self.Ks = nn.Linear(d_model,d_model,bias=True)\n",
    "        self.Vs = nn.Linear(d_model,d_model,bias=True)\n",
    "        self.mha = nn.Linear(d_model,d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
    "        scaled_dot = torch.matmul(Q, torch.transpose(K,dim0=-2,dim1=-1))/m.sqrt(self.d_model)\n",
    "        if mask is not None:\n",
    "            scaled_dot = scaled_dot.masked_fill(mask == 0, -1e9)\n",
    "        score = self.softmax(scaled_dot)\n",
    "        attention = torch.matmul(score,V)\n",
    "        return attention\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, q_in, k_in, v_in, mask):\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            mask = mask.unsqueeze(1)\n",
    "\n",
    "        batch_size = q_in.shape[0]\n",
    "        head_size = int(self.d_model/self.n_head)\n",
    "        Q = self.Qs(q_in).view(batch_size, -1, self.n_head, head_size).transpose(1, 2)\n",
    "        K = self.Ks(k_in).view(batch_size, -1, self.n_head, head_size).transpose(1, 2)\n",
    "        V = self.Vs(v_in).view(batch_size, -1, self.n_head, head_size).transpose(1, 2)\n",
    "\n",
    "       \n",
    "        scaled_dot = self.scaled_dot_product_attention(Q,K,V,mask)\n",
    "        scaled_dot = (\n",
    "            scaled_dot.transpose(1, 2)\n",
    "            .contiguous()\n",
    "            .view(batch_size, -1, self.d_model)\n",
    "        )\n",
    "        output = self.mha(scaled_dot)\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model = config[\"d_model\"], \n",
    "                       n_heads=config[\"n_heads\"], \n",
    "                       hidden_size= config[\"hidden_size\"], \n",
    "                       dropout = config[\"dropout\"]):\n",
    "        super(EncoderLayer,self).__init__()\n",
    "        self.layer_norm = nn.LayerNorm(d_model,config[\"layernorm_eps\"])\n",
    "        self.linear1 = nn.Linear(d_model,hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, d_model)\n",
    "        self.mha = MultiHeadedAttention(n_heads,d_model)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p = dropout)\n",
    "        self.size = d_model\n",
    "\n",
    "    def forward(self, x, src_mask = None):\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.layer_norm(x + self.dropout(self.mha(x, x, x, src_mask)))\n",
    "        x = x + self.dropout(self.linear_block(x))\n",
    "        return x\n",
    "\n",
    "    def linear_block(self, x:Tensor) -> Tensor:\n",
    "        out = self.linear2(self.dropout(self.activation(self.linear1(x))))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model = config[\"d_model\"], \n",
    "                       n_heads=config[\"n_heads\"], \n",
    "                       hidden_size= config[\"hidden_size\"], \n",
    "                       dropout = config[\"dropout\"]):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer_norm = nn.LayerNorm(d_model,config[\"layernorm_eps\"])\n",
    "        self.linear1 = nn.Linear(d_model,hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, d_model)\n",
    "        self.mha_1 = MultiHeadedAttention(n_heads,d_model)\n",
    "        self.mha_2 = MultiHeadedAttention(n_heads,d_model)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p = dropout)\n",
    "        self.size = d_model\n",
    "\n",
    "\n",
    "    def forward(self,x, encoder_in, src_mask = None, trg_mask = None):\n",
    "\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.layer_norm(x + self.dropout(self.mha_1(x, x, x, trg_mask)))\n",
    "        x = self.layer_norm(x + self.dropout(self.mha_2(x, encoder_in, encoder_in, src_mask)))\n",
    "        x = x + self.dropout(self.linear_block(x))\n",
    "       \n",
    "        return x\n",
    "\n",
    "    def linear_block(self, x:Tensor) -> Tensor:\n",
    "        out = self.linear2(self.dropout(self.activation(self.linear1(x))))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"Core encoder is a stack of N layers\"\n",
    "\n",
    "    def __init__(self, N = config[\"n_layers\"]):\n",
    "        super(Encoder, self).__init__()\n",
    "        EncLayer = EncoderLayer()\n",
    "        self.layers = clones(EncLayer, N)\n",
    "        self.norm = nn.LayerNorm(config[\"d_model\"],config[\"layernorm_eps\"])\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"Pass the input (and mask) through each layer in turn.\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"Generic N layer decoder with masking.\"\n",
    "\n",
    "    def __init__(self, N = config[\"n_layers\"]):\n",
    "        super(Decoder, self).__init__()\n",
    "        DecLayer = DecoderLayer()\n",
    "        self.layers = clones(DecLayer, N)\n",
    "        self.norm = nn.LayerNorm(config[\"d_model\"],config[\"layernorm_eps\"])\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, src_mask, tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, trg_vocab_size, padding_idx):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.src_emb = nn.Sequential(Embeddings(src_vocab_size), PositionalEncoding())\n",
    "        self.trg_emb = nn.Sequential(Embeddings(trg_vocab_size), PositionalEncoding())\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "        self.proj = nn.Linear(config[\"d_model\"], trg_vocab_size)\n",
    "        self.padding_idx = padding_idx\n",
    "\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def make_mask(self, seq, seq_type):\n",
    "        if seq_type == \"src\":\n",
    "            mask = (seq != self.padding_idx).unsqueeze(-2)\n",
    "            return mask\n",
    "\n",
    "        else:\n",
    "            pad_mask = (seq != self.padding_idx).unsqueeze(-2)\n",
    "            no_peak_mask = torch.triu(torch.ones((1, seq.shape[1], seq.shape[1])))\n",
    "            no_peak_mask = no_peak_mask.transpose(dim0=-1, dim1 = -2).type_as(seq.data)\n",
    "            mask = no_peak_mask & pad_mask\n",
    "            return mask\n",
    "\n",
    "    def forward(self,src,trg):\n",
    "        src_mask = self.make_mask(src,\"src\")\n",
    "        trg_mask = self.make_mask(trg,\"trg\")\n",
    "        enc_out = self.encoder(self.src_emb(src),src_mask)\n",
    "        dec_out = self.decoder(self.src_emb(trg),enc_out,src_mask,trg_mask)\n",
    "        out = self.proj(dec_out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Data(batch_size=config[\"batch_size\"],max_length=config[\"max_length\"])\n",
    "train_loader, val_loader, test_loader = train_data.prepare_data()\n",
    "padding_idx, src_size, trg_size = train_data.get_properties()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rate(step, model_size, factor, warmup):\n",
    "    \"\"\"\n",
    "    we have to default the step to 1 for LambdaLR function\n",
    "    to avoid zero raising to negative power.\n",
    "    \"\"\"\n",
    "    if step == 0:\n",
    "        step = 1\n",
    "    return factor * (\n",
    "        model_size ** (-0.5) * min(step ** (-0.5), step * warmup ** (-1.5))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_one_step(model, optimizer, loss, device, data, mode, trg_size, lr_scheduler = None):\n",
    "    src = data[0].to(device)\n",
    "    trg = data[1].to(device)\n",
    "    trg_input = trg[:, :-1]\n",
    "    ys = trg[:, 1:].reshape(-1)\n",
    "    if mode == 'train':\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(src, trg_input)\n",
    "        loss_ = loss(logits.reshape(-1, trg_size), ys)\n",
    "        loss_.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if lr_scheduler != None:\n",
    "            lr_scheduler.step()\n",
    "\n",
    "    else:\n",
    "        model.eval()\n",
    "        logits = model(src, trg_input)\n",
    "        loss_ = loss(logits.reshape(-1, trg_size), ys)\n",
    "\n",
    "    return loss_.item() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Transformer(src_size, trg_size, padding_idx)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=config[\"base_lr\"], betas=(0.9, 0.98), eps=1e-9\n",
    "    )\n",
    "lr_scheduler = LambdaLR(\n",
    "        optimizer=optimizer,\n",
    "        lr_lambda=lambda step: rate(\n",
    "            step, model_size=512, factor=1.0, warmup=config[\"warmup\"]\n",
    "        ),\n",
    "    )\n",
    "loss = nn.CrossEntropyLoss()\n",
    "max_epoch = 30\n",
    "best_val_loss = 1e9\n",
    "for epoch in range(max_epoch):\n",
    "    train_loss = 0.0\n",
    "    val_loss = 0.0\n",
    "    for batch in tqdm(train_loader):\n",
    "        running_loss = run_one_step(model,optimizer,loss,device,batch,\"train\",trg_size,lr_scheduler)\n",
    "        train_loss += running_loss\n",
    "    train_loss /= len(train_loader)\n",
    "    lr = optimizer.param_groups[0][\"lr\"]\n",
    "    for batch in val_loader:\n",
    "        running_loss = run_one_step(model,optimizer,loss,device,batch,\"val\",trg_size,lr_scheduler)\n",
    "        val_loss += running_loss\n",
    "    val_loss /= len(val_loader)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'saved/model-{0}.pt'.format(val_loss))\n",
    "    print(\"Epoch {}, train loss: {}, val_loss: {}, current_lr: {}\".format(epoch, train_loss, val_loss, lr)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idx_to_word(x, vocab):\n",
    "    words = []\n",
    "    for i in x:\n",
    "        word = vocab.lookup_token(i)\n",
    "        if '<' not in word:\n",
    "            words.append(word)\n",
    "    words = \" \".join(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"saved/model-1.0646797716617584.pt\"))\n",
    "with torch.no_grad():\n",
    "        for i,batch in enumerate(test_loader):\n",
    "                model.eval()\n",
    "                src = batch[0].to(device)\n",
    "                trg = batch[1].to(device)\n",
    "\n",
    "                trg_input = trg[:, :-1]\n",
    "                ys = trg[:, 1:].reshape(-1)\n",
    "                logits = model(src, trg_input)\n",
    "                for j in range(64):\n",
    "                        src_words = idx_to_word(src[j], train_data.vocab_src)\n",
    "                        trg_words = idx_to_word(trg[j], train_data.vocab_trg)\n",
    "                        output_words = logits[j].max(dim=1)[1]\n",
    "                        output_words = idx_to_word(output_words, train_data.vocab_trg)\n",
    "\n",
    "                        print('source :', src_words)\n",
    "                        print('target :', trg_words)\n",
    "                        print('predicted :', output_words)\n",
    "                "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "32e212cd59109d51c5e62eb067136b47cc7919e85e411c42c9727647d63e8bfc"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 ('dl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
